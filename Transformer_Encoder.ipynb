{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a5570c-648f-4d38-a3dd-bea76339a71f",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "![Alt Text](b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb9454-6635-4e8b-8fd6-a4696c16e4a1",
   "metadata": {},
   "source": [
    "### Input Embedding:\n",
    "The input to the encoder is first passed through an embedding layer that converts each token into a fixed-size dense vector representation.\n",
    "\n",
    "### Positional Encoding:\n",
    "Since the Transformer architecture doesn’t have a sense of word order, positional encoding is added to the input embeddings to give the model information about the position of each token in the sequence.\n",
    "\n",
    "### Multi-Head Attention:\n",
    "Multi-head attention allows the model to focus on different parts of the input sequence simultaneously. It involves multiple self-attention mechanisms, allowing the model to capture relationships between words irrespective of their positions.\n",
    "\n",
    "### Add & Norm:\n",
    "A residual connection is added around the multi-head attention and feed-forward layers followed by layer normalization to stabilize training.\n",
    "\n",
    "### Feed-Forward Network:\n",
    "A fully connected feed-forward network is applied after the attention layer, which processes each token individually.\n",
    "\n",
    "### Stacking Layers (Nx):\n",
    "The above operations (multi-head attention, feed-forward) are repeated N times, where  N is the number of layers in the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b152d59-0892-4552-a935-7721e685cbad",
   "metadata": {},
   "source": [
    "### 1.Tokenization & Input Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "812b2596-f969-468b-b972-fcc1a715d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erginous/anaconda3/envs/RAG/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"\"\"\n",
    " The training process for language models (i.e., both encoder-only and decoder-only models) includes pretraining and finetuning. During pretraining, we train the model via a self-supervised objective over a large amount of unlabeled text. Although pretraining is expensive, \n",
    " we can reuse the resulting model numerous times as a starting point for finetuning on various tasks. Due to the public availability of many high-quality pretrained LLMs, most practitioners can simply download a pretrained model and focus upon the finetuning process without ever having to pretrain a model from scratch.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs= tokenizer(text , return_tensors = \"pt\",padding=True,truncation=True)  # \"pt\" - return input_ids pytorch tensor , \n",
    "#truncation=True: Automatically truncates sequences longer than the model’s maximum input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08044fc8-a4a1-45d0-99b5-b889ce011493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1996,  2731,  2832,  2005,  2653,  4275,  1006,  1045,  1012,\n",
       "          1041,  1012,  1010,  2119,  4372, 16044,  2099,  1011,  2069,  1998,\n",
       "         21933,  4063,  1011,  2069,  4275,  1007,  2950,  3653, 23654,  2075,\n",
       "          1998,  2986,  8525,  5582,  1012,  2076,  3653, 23654,  2075,  1010,\n",
       "          2057,  3345,  1996,  2944,  3081,  1037,  2969,  1011, 13588,  7863,\n",
       "          2058,  1037,  2312,  3815,  1997,  4895, 20470, 12260,  2094,  3793,\n",
       "          1012,  2348,  3653, 23654,  2075,  2003,  6450,  1010,  2057,  2064,\n",
       "          2128,  8557,  1996,  4525,  2944,  3365,  2335,  2004,  1037,  3225,\n",
       "          2391,  2005,  2986,  8525,  5582,  2006,  2536,  8518,  1012,  2349,\n",
       "          2000,  1996,  2270, 11343,  1997,  2116,  2152,  1011,  3737,  3653,\n",
       "         23654,  2098,  2222,  5244,  1010,  2087, 14617,  2064,  3432,  8816,\n",
       "          1037,  3653, 23654,  2098,  2944,  1998,  3579,  2588,  1996,  2986,\n",
       "          8525,  5582,  2832,  2302,  2412,  2383,  2000,  3653, 23654,  1037,\n",
       "          2944,  2013, 11969,  1012,   102]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = inputs['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0da59c7a-f601-404b-90a7-d69ff8427a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size , d_model):\n",
    "        super(EmbeddingLayer,self).__init__() \n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "vocab_size =50000\n",
    "d_model = 512 # Dimension  of embedding\n",
    "\n",
    "embedding_layer = EmbeddingLayer(vocab_size,d_model)\n",
    "embedded_input = embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7a619060-f387-4118-b232-fb9a2b5a7c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 512])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_input.shape) # 135 - vocabulary size and 512 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c515f3-2a0c-4139-b4b5-8622a49f6853",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740738f3-f8dd-441c-9370-347d5929d311",
   "metadata": {},
   "source": [
    "![Alt Text](a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "364c7889-3a49-4383-a4dd-6be7efd99214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model ,max_len =5000):\n",
    "        super(PositionalEncoding,self).__init__() \n",
    "        pe = torch.zeros(max_len,d_model)  # 5000,512\n",
    "        position = torch.arange(0,max_len,dtype = torch.float).unsqueeze(1)  # 5000,1\n",
    "        div_term = torch.exp(torch.arange(0,d_model , 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position  * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x+self.pe[:,:x.size(1),:]\n",
    "        return x\n",
    "\n",
    "pos_encoding_layer = PositionalEncoding(d_model)\n",
    "positioned_input = pos_encoding_layer(embedded_input)  # 1, 135,512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ee892f3-a330-4816-a3be-934ef9167a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 135, 512])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positioned_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff4f38-2b93-45d2-bd16-eb8318c78c21",
   "metadata": {},
   "source": [
    "### 3. Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b3e9fcd5-37a1-4098-a0c0-94ba92a738f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 512])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model , num_heads):\n",
    "        super(MultiHeadAttention,self).__init__() \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.query = nn.Linear(d_model,d_model)\n",
    "        self.key= nn.Linear(d_model,d_model)\n",
    "        self.value = nn.Linear(d_model,d_model)\n",
    "        self.fc_out = nn.Linear(d_model,d_model)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        N = query.shape[0]\n",
    "        query_len,key_len ,value_len =  query.shape[1] , key.shape[1] , value.shape[1]\n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        query = self.query(query).view(N, query_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.key(key).view(N, key_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = self.value(value).view(N, value_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        out = torch.matmul(attention, value)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.d_model)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "num_heads = 8\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "attention_out = multi_head_attention(positioned_input, positioned_input, positioned_input)\n",
    "print(attention_out.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "42501583-3c54-4cb8-9b0c-fd6f3c328e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0280, -0.0293,  0.0969,  ..., -0.3313,  0.2431, -0.2711],\n",
       "         [-0.0512, -0.0586,  0.1172,  ..., -0.3182,  0.2282, -0.2613],\n",
       "         [-0.0291, -0.0266,  0.0683,  ..., -0.3149,  0.2242, -0.2864],\n",
       "         ...,\n",
       "         [-0.0049, -0.0351,  0.0683,  ..., -0.3615,  0.2324, -0.2865],\n",
       "         [-0.0193, -0.0222,  0.0572,  ..., -0.3455,  0.2195, -0.2909],\n",
       "         [-0.0104, -0.0133,  0.0789,  ..., -0.3309,  0.2186, -0.2976]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef43038-3c51-44a2-86d8-3904e3a4cb58",
   "metadata": {},
   "source": [
    "### 4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c5a8da5-9ec8-4a26-8277-22849983acae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 512])\n"
     ]
    }
   ],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self , d_model):\n",
    "        super(AddNorm,self).__init__() \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self,x,sublayer):\n",
    "        return self.norm(x+sublayer)\n",
    "\n",
    "add_norm =AddNorm(d_model)\n",
    "attention_with_norm = add_norm(positioned_input, attention_out)\n",
    "print(attention_with_norm.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78f5d8-709b-439c-9a98-cf41e45c727d",
   "metadata": {},
   "source": [
    "### 5. Feed Forward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a9d7ace5-89ac-4c54-8a82-5d16b60965a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 512])\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# Example usage\n",
    "d_ff = 2048\n",
    "feed_forward = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "ffn_out = feed_forward(attention_with_norm)\n",
    "print(ffn_out.shape)  # Output shape: (1, 10, 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c9e98-d912-4033-8dd7-1f15a80ce17a",
   "metadata": {},
   "source": [
    "### 6. Complete Encoder Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8635be58-c175-4f13-932f-39ec0565dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 512])\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.add_norm1 = AddNorm(d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.add_norm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        out1 = self.add_norm1(x, attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.add_norm2(out1, ffn_output)\n",
    "        return out2\n",
    "\n",
    "# Example usage\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "output = encoder_layer(positioned_input)\n",
    "print(output.shape)  # Output shape: (1, 10, 512)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
