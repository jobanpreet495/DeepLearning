{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20bb389-c961-4e12-8723-4aa72954ad4b",
   "metadata": {},
   "source": [
    "## Coding RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f03d5a-b17d-4b88-9f63-dfa838b0f65e",
   "metadata": {},
   "source": [
    "To build an RNN (Recurrent Neural Network) architecture from scratch for a simple text generation task using PyTorch, we'll take the approach of predicting the next word in a sequence based on previous words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021dc2f-7ed9-4a2d-a8ae-9a83b492551c",
   "metadata": {},
   "source": [
    "#### A Recurrent Neural Network (RNN) \n",
    "\n",
    "    • Type of neural network designed to handle sequential data by maintaining a memory of previous inputs.\n",
    "    • Traditional feedforward neural networks,  process input independently \n",
    "    • RNNs have loops that allow information to persist. \n",
    "    • Particularly suited for tasks involving sequences, such as time series analysis, language modeling, or speech recognition.\n",
    "    • Hidden State: RNN maintains a hidden state, which serves as memory and carries information from previous time steps.\n",
    "    \n",
    "Mathematically: ht=tanh(Wh⋅ht−1+Wx⋅xt) Where:\n",
    "\n",
    "    • ht is the hidden state at time t.\n",
    "    • xt is the input at time t.\n",
    "    • Wh and Wx are weight matrices.\n",
    "    • tanh is a non-linear activation function.\n",
    "    \n",
    "####  Problems with RNN\n",
    "* Vanishing gradients: During backpropagation, gradients can shrink exponentially as they are propagated backward through time. This makes it difficult for RNNs to learn long-term dependencies.\n",
    "* Slow Training: Due to sequential nature of input processing .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991dc000-bcb5-4004-a9b5-d27f65ef1fed",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16925cd-0932-458d-9546-a8ccab6bcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e953a6-139b-46e0-a47b-3ffb5e5c4b4a",
   "metadata": {},
   "source": [
    "### Import  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a48197f-4cda-4d0a-8ffa-1ee0740c665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658d327-578c-481c-acba-dd7eabd1b924",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a8aa46-2b03-45ed-a6c3-f2066e58c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Recurrent neural networks are a type of neural network. They are used for tasks that involve sequences, \n",
    "like text generation and time series prediction. An RNN works by maintaining a hidden state that updates \n",
    "with each new input.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8805e8-0fc6-4039-9a6c-c7154641b90b",
   "metadata": {},
   "source": [
    "* Now We'll split the paragraph into individual words and create a vocabulary (word-to-index and index-to-word mappings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c24d41b-e533-480f-8e63-2a27a1eb5901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 34\n"
     ]
    }
   ],
   "source": [
    "words = text.lower().split() # Split text into words\n",
    "vocab =set(words) # vocabulary\n",
    "\n",
    "# creating mapping for word2idx and idx2word\n",
    "word_to_idx = {word:i for i , word in enumerate(vocab)}\n",
    "idx_to_word = {i:word for i,word in enumerate(vocab)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e72e0-9d3d-4767-b90b-6b074c3135ce",
   "metadata": {},
   "source": [
    "### Prepare data for  Training\n",
    "\n",
    "* RNNs work with sequences of inputs. For this, we'll create input sequences (X) and their corresponding target outputs (Y). Each X will be a sequence of words, and Y will be the word following that sequence.\n",
    "* For simplicity, we'll take a window size of 3 (3 words to predict the next word).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fb4446-98e6-4b1b-b10f-09a3bafa207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(words , word_to_idx , seq_length=3):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(words)-seq_length):\n",
    "        seq = words[i:i+seq_length]  \n",
    "        target = words[i+seq_length]\n",
    "        sequences.append([word_to_idx[w] for w in seq])\n",
    "        targets.append(word_to_idx[target])\n",
    "\n",
    "    return torch.tensor(sequences) , torch.tensor(targets)\n",
    "\n",
    "seq_length =3    # Predict the next word based on previous 3 words\n",
    "X , Y = create_sequences(words  , word_to_idx , seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e75e06-6d39-4ca4-a1ae-e7b5f9eb12f8",
   "metadata": {},
   "source": [
    "### Define the RNN model\n",
    "Now we’ll define a simple RNN model using PyTorch's nn.Module. An RNN has three key components:\n",
    "\n",
    "* Embedding layer: Converts the input word indices into dense vector representations.\n",
    "* RNN layer: Handles the recurrent connections.\n",
    "* Fully connected (linear) layer: Maps the RNN outputs to vocabulary size for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ae35e22-9d14-427b-a979-2160da6cbe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(34, 10)\n",
      "  (rnn): RNN(10, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=34, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self , vocab_size , embed_size , hidden_size , output_size):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        # Simple RNN layer\n",
    "        self.rnn  = nn.RNN(embed_size , hidden_size ,   batch_first = True)\n",
    "        # Fully connected layer to map hidden state to output\n",
    "        self.fc = nn.Linear(hidden_size , output_size)\n",
    "\n",
    "    def forward(self,x , hidden):\n",
    "        # x:  input sequences of word  indices\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # pass through RNN . It outputs the hidden state for each time step\n",
    "        out,hidden = self.rnn(x , hidden)\n",
    "        # Only use the output of the last time step for the final prediction\n",
    "        out = self.fc(out[: , -1 ,:])\n",
    "        return out ,   hidden\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        #Initialize hidden state  with zeros\n",
    "        return torch.zeros(1 ,batch_size,hidden_size)\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 10  # Embedding size for each word\n",
    "hidden_size = 20  # Number of hidden units in RNN\n",
    "output_size = vocab_size  # Output size equals vocabulary size\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleRNN(vocab_size, embed_size, hidden_size, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff35bb-516a-4fef-86a7-78e28d19f3c8",
   "metadata": {},
   "source": [
    "### 6. Training the model\n",
    "* For training, we will use the cross-entropy loss function, which is suitable for multi-class classification problems (like predicting one word out of the entire vocabulary). We'll also use the Adam optimizer for updating the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9e5516a-1fb0-4c8e-9be9-2320682a7fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.2728\n",
      "Epoch [200/500], Loss: 0.1130\n",
      "Epoch [300/500], Loss: 0.0618\n",
      "Epoch [400/500], Loss: 0.0398\n",
      "Epoch [500/500], Loss: 0.0282\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters() , lr=0.001)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs=500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    hidden = model.init_hidden(X.size(0))\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad() \n",
    "    # Forward pass\n",
    "    output , hidden = model(X,hidden)\n",
    "    # Compute loss\n",
    "    loss = criterion(output , Y)\n",
    "    # backward pass  and optimization\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e7716b9-7ccb-46d7-aab6-b513d3035731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are a type of neural generation and of neural network. "
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, input_seq, hidden):\n",
    "    with torch.no_grad():\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "        return predicted_idx, hidden\n",
    "\n",
    "# Start the sequence with an initial set of words\n",
    "initial_words = ['recurrent','neural', 'networks']\n",
    "input_seq = torch.tensor([[word_to_idx[word] for word in initial_words]])\n",
    "\n",
    "# Generate the next 10 words\n",
    "model.eval()\n",
    "hidden = model.init_hidden(input_seq.size(0))\n",
    "\n",
    "for _ in range(10):\n",
    "    predicted_idx, hidden = predict_next_word(model, input_seq, hidden)\n",
    "    predicted_word = idx_to_word[predicted_idx]\n",
    "    \n",
    "    # Print the generated word\n",
    "    print(predicted_word, end=' ')\n",
    "    \n",
    "    # Update input sequence by removing the first word and adding the predicted word\n",
    "    input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_idx]])], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765ec73-5383-489f-a9b6-571c74723a1e",
   "metadata": {},
   "source": [
    "### Hadling out of vocabulary problem using BERT tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6bb72-954c-4a92-acdd-0e3910012549",
   "metadata": {},
   "source": [
    "Handling out-of-vocabulary (OOV) words using a pre-trained tokenizer like BertTokenizer from the Hugging Face transformers library. The BERT tokenizer uses WordPiece tokenization, which helps split unseen words into subword units rather than marking them as OOV or <UNK>.\n",
    "\n",
    "Let’s walk through how the BERT tokenizer handles unseen words and how it can be used with your create_sequences function.\n",
    "\n",
    "#### Key Points:\n",
    "WordPiece Tokenization: BERT uses WordPiece tokenization, meaning if a word is not in the vocabulary, it will be split into subwords. This makes it more robust to unseen or rare words.\n",
    "Example: The word unseen_word would be broken down into subword tokens like un##seen and ##_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63a97d45-14ac-4e85-9eb2-3cc67f5dab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the sentence using BERT tokenizer\n",
    "tokenized_sentence = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "# Create sequences and targets for next-word prediction\n",
    "def create_sequences_bert(tokenized_sentence, seq_length=3):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    # Generate sequences and their corresponding next token (target)\n",
    "    for i in range(len(tokenized_sentence) - seq_length):\n",
    "        seq = tokenized_sentence[i:i + seq_length]  # Current sequence of token IDs\n",
    "        target = tokenized_sentence[i + seq_length]  # Next token ID to predict\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    return torch.tensor(sequences), torch.tensor(targets)\n",
    "\n",
    "# Define sequence length\n",
    "seq_length = 3  # Predict the next token based on the previous 3 tokens\n",
    "\n",
    "# Generate sequences and targets using BERT tokenization\n",
    "X, Y = create_sequences_bert(tokenized_sentence, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "648eb960-c546-4269-b25c-76cedf52f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(30522, 10)\n",
      "  (rnn): RNN(10, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=30522, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self , vocab_size , embed_size , hidden_size , output_size):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        # Simple RNN layer\n",
    "        self.rnn  = nn.RNN(embed_size , hidden_size ,   batch_first = True)\n",
    "        # Fully connected layer to map hidden state to output\n",
    "        self.fc = nn.Linear(hidden_size , output_size)\n",
    "\n",
    "    def forward(self,x , hidden):\n",
    "        # x:  input sequences of word  indices\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # pass through RNN . It outputs the hidden state for each time step\n",
    "        out,hidden = self.rnn(x , hidden)\n",
    "        # Only use the output of the last time step for the final prediction\n",
    "        out = self.fc(out[: , -1 ,:])\n",
    "        return out ,   hidden\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        #Initialize hidden state  with zeros\n",
    "        return torch.zeros(1 ,batch_size,hidden_size)\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 10  # Embedding size for each word\n",
    "hidden_size = 20  # Number of hidden units in RNN\n",
    "vocab_size = tokenizer.vocab_size\n",
    "output_size = vocab_size  # Output size equals vocabulary size\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleRNN(vocab_size, embed_size, hidden_size, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74c61c20-7219-47f7-8fdc-a1e66be6c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.0435\n",
      "Epoch [200/500], Loss: 0.0227\n",
      "Epoch [300/500], Loss: 0.0133\n",
      "Epoch [400/500], Loss: 0.0087\n",
      "Epoch [500/500], Loss: 0.0063\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters() , lr=0.001)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs=500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    hidden = model.init_hidden(X.size(0))\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad() \n",
    "    # Forward pass\n",
    "    output , hidden = model(X,hidden)\n",
    "    # Compute loss\n",
    "    loss = criterion(output , Y)\n",
    "    # backward pass  and optimization\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e157ffe2-3bf4-41ea-b891-323326499d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a type of neural networks a a type of "
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, input_seq, hidden):\n",
    "    with torch.no_grad():\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        # Get the predicted token index (the word with the highest score)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "        return predicted_idx, hidden\n",
    "\n",
    "# Start the sequence with an initial set of words\n",
    "initial_words = ['recurrent', 'neural', 'networks']\n",
    "\n",
    "# Convert the words to their token indices using the tokenizer\n",
    "input_seq = torch.tensor([tokenizer.encode(' '.join(initial_words), add_special_tokens=False)])\n",
    "\n",
    "# Generate the next 10 words\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "hidden = model.init_hidden(input_seq.size(0))  # Initialize hidden state\n",
    "\n",
    "# Loop to predict the next 10 words\n",
    "for _ in range(10):\n",
    "    predicted_idx, hidden = predict_next_word(model, input_seq, hidden)\n",
    "    \n",
    "    # Convert the predicted token index back to a word\n",
    "    predicted_word = tokenizer.decode([predicted_idx])\n",
    "    \n",
    "    # Print the predicted word\n",
    "    print(predicted_word, end=' ')\n",
    "    \n",
    "    # Update the input sequence by removing the first token and adding the predicted token\n",
    "    input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_idx]])], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3cd61-ba6d-48d7-a079-436726613dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
